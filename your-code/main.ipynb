{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:24:13.216397Z",
     "start_time": "2020-02-11T21:24:12.197383Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# from pprint import pprint\n",
    "# from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "# import urllib.request\n",
    "# from urllib.request import urlopen\n",
    "# import random\n",
    "# import re\n",
    "# import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:24:13.227037Z",
     "start_time": "2020-02-11T21:24:13.220444Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:24:14.467021Z",
     "start_time": "2020-02-11T21:24:13.233785Z"
    }
   },
   "outputs": [],
   "source": [
    "#your code\n",
    "html=requests.get(url).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:24:14.765285Z",
     "start_time": "2020-02-11T21:24:14.471147Z"
    }
   },
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(html,'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:39:29.184679Z",
     "start_time": "2020-02-11T21:39:29.180039Z"
    }
   },
   "outputs": [],
   "source": [
    "#tags = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:42:21.591196Z",
     "start_time": "2020-02-11T21:42:21.486582Z"
    }
   },
   "outputs": [],
   "source": [
    "names = soup.find_all(['h1'],{'class':['h3 lh-condensed']})\n",
    "users = soup.find_all(['p'],{'class':['f4 text-normal mb-1']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:42:22.817674Z",
     "start_time": "2020-02-11T21:42:22.811551Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"users_and_names = soup.find_all(['h1','p'],{'class':['h3 lh-condensed','f4 text-normal mb-1']})\\ntest_names=[i for i in users_and_names]\\ntest_names = [name.text.strip().split('\\n') for name in test_names]\\ntest_names\""
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''users_and_names = soup.find_all(['h1','p'],{'class':['h3 lh-condensed','f4 text-normal mb-1']})\n",
    "test_names=[i for i in users_and_names]\n",
    "test_names = [name.text.strip().split('\\n') for name in test_names]\n",
    "test_names'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:42:23.976457Z",
     "start_time": "2020-02-11T21:42:23.970516Z"
    }
   },
   "outputs": [],
   "source": [
    "names2 = [i for i in names]\n",
    "names = [name.text.strip().split('\\n') for name in names2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:40:44.511770Z",
     "start_time": "2020-02-11T21:40:44.505473Z"
    }
   },
   "outputs": [],
   "source": [
    "usernames=[i for i in users]\n",
    "usernames = [username.text.strip().split('\\n') for username in usernames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:40:47.799731Z",
     "start_time": "2020-02-11T21:40:47.780838Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-1b0c656b40dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mname_username\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' ('\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0musernames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m')'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mname_username\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mname_username\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#your code\n",
    "name_username=[]\n",
    "for i in range(len(names)):\n",
    "    x=names[i][0]+' ('+usernames[i][0]+')'\n",
    "    name_username.append(x)\n",
    "name_username"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:42:38.750095Z",
     "start_time": "2020-02-11T21:42:38.741152Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:42:42.907025Z",
     "start_time": "2020-02-11T21:42:40.043716Z"
    }
   },
   "outputs": [],
   "source": [
    "#your code\n",
    "html=requests.get(url).content\n",
    "soup=BeautifulSoup(html,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:42:44.590260Z",
     "start_time": "2020-02-11T21:42:44.449361Z"
    }
   },
   "outputs": [],
   "source": [
    "repos = soup.find_all(['h1'],{'class':['h3 lh-condensed']})\n",
    "names = soup.find_all(['p'],{'class':['col-9 text-gray my-1 pr-4']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:42:46.360905Z",
     "start_time": "2020-02-11T21:42:46.354214Z"
    }
   },
   "outputs": [],
   "source": [
    "names=[i for i in names]\n",
    "names = [name.text.strip().split('\\n') for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:42:47.897942Z",
     "start_time": "2020-02-11T21:42:47.892807Z"
    }
   },
   "outputs": [],
   "source": [
    "repos=[i for i in repos]\n",
    "repos = [repo.text.strip().split('\\n') for repo in repos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:42:49.585853Z",
     "start_time": "2020-02-11T21:42:49.573708Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['REPO: brettkromkamp /       contextualise NAME:  (Contextualise is a simple and flexible tool particularly suited for organising information-heavy projects and activities consisting of unstructured and widely diverse data and information resources)',\n",
       " 'REPO: eriklindernoren /       ML-From-Scratch NAME:  (Machine Learning From Scratch. Bare bones NumPy implementations of machine learning models and algorithms with a focus on accessibility. Aims to cover everything from linear regression to deep learning.)',\n",
       " 'REPO: domlysz /       BlenderGIS NAME:  (Blender addons to make the bridge between Blender and geographic data)',\n",
       " 'REPO: cycz /       jdBuyMask NAME:  (京东监控口罩有货爬虫，自动下单爬虫，口罩爬虫)',\n",
       " 'REPO: xingyizhou /       CenterNet NAME:  (Object detection, 3D detection, and pose estimation using center point detection:)',\n",
       " 'REPO: tlbootcamp /       tlroadmap NAME:  (👩🏼\\u200d💻👨🏻\\u200d💻Карта навыков и модель развития тимлидов)',\n",
       " 'REPO: albumentations-team /       albumentations NAME:  (fast image augmentation library and easy to use wrapper around other libraries)',\n",
       " 'REPO: mingrammer /       diagrams NAME:  (🎨 Diagram as Code for prototyping cloud system architectures)',\n",
       " 'REPO: pypa /       virtualenv NAME:  (Virtual Python Environment builder)',\n",
       " 'REPO: Rlacat /       jd-automask NAME:  (防护-京东口罩自动抢购并下单)',\n",
       " 'REPO: kevinzakka /       nca NAME:  (A PyTorch implementation of Neighbourhood Components Analysis.)',\n",
       " 'REPO: robotframework /       robotframework NAME:  (Generic automation framework for acceptance testing and RPA)',\n",
       " 'REPO: python /       mypy NAME:  (Optional static typing for Python 3 and 2 (PEP 484))',\n",
       " 'REPO: openai /       gym NAME:  (A toolkit for developing and comparing reinforcement learning algorithms.)',\n",
       " 'REPO: explosion /       spaCy NAME:  (💫 Industrial-strength Natural Language Processing (NLP) with Python and Cython)',\n",
       " 'REPO: tensortrade-org /       tensortrade NAME:  (An open source reinforcement learning framework for training, evaluating, and deploying robust trading agents.)',\n",
       " 'REPO: fendouai /       PyTorchDocs NAME:  (PyTorch 官方中文教程包含 60 分钟快速入门教程，强化教程，计算机视觉，自然语言处理，生成对抗网络，强化学习。欢迎 Star，Fork！)',\n",
       " 'REPO: PrefectHQ /       prefect NAME:  (The Prefect Core workflow engine)',\n",
       " 'REPO: nodejs /       node-gyp NAME:  (Node.js native addon build tool)',\n",
       " 'REPO: satoshiiizuka /       siggraphasia2019_remastering NAME:  (Code for the paper \"DeepRemaster: Temporal Source-Reference Attention Networks for Comprehensive Video Enhancement\". http://iizuka.cs.tsukuba.ac.jp/projects/remastering/)',\n",
       " 'REPO: clovaai /       CRAFT-pytorch NAME:  (Official implementation of Character Region Awareness for Text Detection (CRAFT))',\n",
       " 'REPO: programthink /       zhao NAME:  (【编程随想】整理的《太子党关系网络》，专门揭露赵国的权贵)',\n",
       " 'REPO: tensorflow /       models NAME:  (Models and examples built with TensorFlow)',\n",
       " 'REPO: keras-team /       keras NAME:  (Deep Learning for humans)',\n",
       " 'REPO: cloud-custodian /       cloud-custodian NAME:  (Rules engine for cloud security, cost optimization, and governance, DSL in yaml for policies to query, filter, and take actions on resources)']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_names=[]\n",
    "for i in range(len(repos)):\n",
    "    x='REPO: '+repos[i][0]+' '+repos[i][4]+' NAME: '' ('+names[i][0]+')'\n",
    "    repo_names.append(x)\n",
    "repo_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:42:55.042039Z",
     "start_time": "2020-02-11T21:42:55.037922Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:42:57.162688Z",
     "start_time": "2020-02-11T21:42:56.592654Z"
    }
   },
   "outputs": [],
   "source": [
    "#your code\n",
    "html=requests.get(url).content\n",
    "soup=BeautifulSoup(html,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:42:58.801138Z",
     "start_time": "2020-02-11T21:42:58.798703Z"
    }
   },
   "outputs": [],
   "source": [
    "#repos = soup.find_all('a',{'class':['image']})\n",
    "#repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:43:00.439006Z",
     "start_time": "2020-02-11T21:43:00.428607Z"
    }
   },
   "outputs": [],
   "source": [
    "images = soup.find_all('img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:43:01.806009Z",
     "start_time": "2020-02-11T21:43:01.796937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http:///upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png',\n",
       " 'http:///upload.wikimedia.org/wikipedia/en/thumb/1/1b/Semi-protection-shackle.svg/20px-Semi-protection-shackle.svg.png',\n",
       " 'http:///upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG',\n",
       " 'http:///upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png',\n",
       " 'http:///upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg',\n",
       " 'http:///upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Newman_Laugh-O-Gram_%281921%29.webm/220px-seek%3D2-Newman_Laugh-O-Gram_%281921%29.webm.jpg',\n",
       " 'http:///upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg',\n",
       " 'http:///upload.wikimedia.org/wikipedia/commons/thumb/7/71/Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg/170px-Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg',\n",
       " 'http:///upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg',\n",
       " 'http:///upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg',\n",
       " 'http:///upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg',\n",
       " 'http:///upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg',\n",
       " 'http:///upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg',\n",
       " 'http:///upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg',\n",
       " 'http:///upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg',\n",
       " 'http:///upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG',\n",
       " 'http:///upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Roy_O._Disney_with_Company_at_Press_Conference.jpg/170px-Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n",
       " 'http:///upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Disney_Display_Case.JPG/170px-Disney_Display_Case.JPG',\n",
       " 'http:///upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg',\n",
       " 'http:///upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " 'http:///upload.wikimedia.org/wikipedia/commons/thumb/4/44/The_Walt_Disney_Company_Logo.svg/120px-The_Walt_Disney_Company_Logo.svg.png',\n",
       " 'http:///upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/30px-Animation_disc.svg.png',\n",
       " 'http:///upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/29px-P_vip.svg.png',\n",
       " 'http:///upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Magic_Kingdom_castle.jpg/24px-Magic_Kingdom_castle.jpg',\n",
       " 'http:///upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/30px-Video-x-generic.svg.png',\n",
       " 'http:///upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/30px-Flag_of_Los_Angeles_County%2C_California.svg.png',\n",
       " 'http:///upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/30px-Blank_television_set.svg.png',\n",
       " 'http:///upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/30px-Flag_of_the_United_States.svg.png',\n",
       " 'http:///upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/22px-Commons-logo.svg.png',\n",
       " 'http:///upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/25px-Wikiquote-logo.svg.png',\n",
       " 'http:///upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/30px-Wikidata-logo.svg.png',\n",
       " 'http:///upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " 'http:///en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1',\n",
       " 'http://static/images/wikimedia-button.png',\n",
       " 'http://static/images/poweredby_mediawiki_88x31.png']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_imag=[]\n",
    "for image in images:\n",
    "    list_imag.append('http:/'+image.get('src'))\n",
    "    \n",
    "list_imag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:43:04.932832Z",
     "start_time": "2020-02-11T21:43:04.928918Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:43:07.083891Z",
     "start_time": "2020-02-11T21:43:06.799687Z"
    }
   },
   "outputs": [],
   "source": [
    "#your code\n",
    "html=requests.get(url).content\n",
    "soup=BeautifulSoup(html,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:43:07.983064Z",
     "start_time": "2020-02-11T21:43:07.977629Z"
    }
   },
   "outputs": [],
   "source": [
    "links = soup.find_all('a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:43:10.677973Z",
     "start_time": "2020-02-11T21:43:10.672092Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "link=[]\n",
    "for i in links:\n",
    "    link.append(i.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:43:12.348875Z",
     "start_time": "2020-02-11T21:43:12.333990Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://en.wiktionary.org/wiki/Python',\n",
       " 'https://en.wiktionary.org/wiki/python',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Special:WhatLinksHere/Python&namespace=0',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&oldid=937840393',\n",
       " 'https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en',\n",
       " 'https://www.wikidata.org/wiki/Special:EntityPage/Q747452',\n",
       " 'https://commons.wikimedia.org/wiki/Category:Python',\n",
       " 'https://af.wikipedia.org/wiki/Python',\n",
       " 'https://als.wikipedia.org/wiki/Python',\n",
       " 'https://ar.wikipedia.org/wiki/%D8%A8%D8%A7%D9%8A%D8%AB%D9%88%D9%86',\n",
       " 'https://az.wikipedia.org/wiki/Python',\n",
       " 'https://bn.wikipedia.org/wiki/%E0%A6%AA%E0%A6%BE%E0%A6%87%E0%A6%A5%E0%A6%A8_(%E0%A6%A6%E0%A7%8D%E0%A6%AC%E0%A7%8D%E0%A6%AF%E0%A6%B0%E0%A7%8D%E0%A6%A5%E0%A6%A4%E0%A6%BE_%E0%A6%A8%E0%A6%BF%E0%A6%B0%E0%A6%B8%E0%A6%A8)',\n",
       " 'https://be.wikipedia.org/wiki/Python',\n",
       " 'https://bg.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%BF%D0%BE%D1%8F%D1%81%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5)',\n",
       " 'https://cs.wikipedia.org/wiki/Python_(rozcestn%C3%ADk)',\n",
       " 'https://da.wikipedia.org/wiki/Python',\n",
       " 'https://de.wikipedia.org/wiki/Python',\n",
       " 'https://eo.wikipedia.org/wiki/Pitono_(apartigilo)',\n",
       " 'https://eu.wikipedia.org/wiki/Python_(argipena)',\n",
       " 'https://fa.wikipedia.org/wiki/%D9%BE%D8%A7%DB%8C%D8%AA%D9%88%D9%86',\n",
       " 'https://fr.wikipedia.org/wiki/Python',\n",
       " 'https://ko.wikipedia.org/wiki/%ED%8C%8C%EC%9D%B4%EC%84%A0',\n",
       " 'https://hr.wikipedia.org/wiki/Python_(razdvojba)',\n",
       " 'https://io.wikipedia.org/wiki/Pitono',\n",
       " 'https://id.wikipedia.org/wiki/Python',\n",
       " 'https://ia.wikipedia.org/wiki/Python_(disambiguation)',\n",
       " 'https://is.wikipedia.org/wiki/Python_(a%C3%B0greining)',\n",
       " 'https://it.wikipedia.org/wiki/Python_(disambigua)',\n",
       " 'https://he.wikipedia.org/wiki/%D7%A4%D7%99%D7%AA%D7%95%D7%9F',\n",
       " 'https://ka.wikipedia.org/wiki/%E1%83%9E%E1%83%98%E1%83%97%E1%83%9D%E1%83%9C%E1%83%98_(%E1%83%9B%E1%83%A0%E1%83%90%E1%83%95%E1%83%90%E1%83%9A%E1%83%9B%E1%83%9C%E1%83%98%E1%83%A8%E1%83%95%E1%83%9C%E1%83%94%E1%83%9A%E1%83%9D%E1%83%95%E1%83%90%E1%83%9C%E1%83%98)',\n",
       " 'https://kg.wikipedia.org/wiki/Mboma_(nyoka)',\n",
       " 'https://la.wikipedia.org/wiki/Python_(discretiva)',\n",
       " 'https://lb.wikipedia.org/wiki/Python',\n",
       " 'https://hu.wikipedia.org/wiki/Python_(egy%C3%A9rtelm%C5%B1s%C3%ADt%C5%91_lap)',\n",
       " 'https://mr.wikipedia.org/wiki/%E0%A4%AA%E0%A4%BE%E0%A4%AF%E0%A4%A5%E0%A5%89%E0%A4%A8_(%E0%A4%86%E0%A4%9C%E0%A5%8D%E0%A4%9E%E0%A4%BE%E0%A4%B5%E0%A4%B2%E0%A5%80_%E0%A4%AD%E0%A4%BE%E0%A4%B7%E0%A4%BE)',\n",
       " 'https://nl.wikipedia.org/wiki/Python',\n",
       " 'https://ja.wikipedia.org/wiki/%E3%83%91%E3%82%A4%E3%82%BD%E3%83%B3',\n",
       " 'https://no.wikipedia.org/wiki/Pyton',\n",
       " 'https://pl.wikipedia.org/wiki/Pyton',\n",
       " 'https://pt.wikipedia.org/wiki/Python_(desambigua%C3%A7%C3%A3o)',\n",
       " 'https://ru.wikipedia.org/wiki/Python_(%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F)',\n",
       " 'https://sk.wikipedia.org/wiki/Python',\n",
       " 'https://sr.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%B2%D0%B8%D1%88%D0%B5%D0%B7%D0%BD%D0%B0%D1%87%D0%BD%D0%B0_%D0%BE%D0%B4%D1%80%D0%B5%D0%B4%D0%BD%D0%B8%D1%86%D0%B0)',\n",
       " 'https://sh.wikipedia.org/wiki/Python',\n",
       " 'https://fi.wikipedia.org/wiki/Python',\n",
       " 'https://sv.wikipedia.org/wiki/Pyton',\n",
       " 'https://th.wikipedia.org/wiki/%E0%B9%84%E0%B8%9E%E0%B8%97%E0%B8%AD%E0%B8%99',\n",
       " 'https://tr.wikipedia.org/wiki/Python',\n",
       " 'https://uk.wikipedia.org/wiki/%D0%9F%D1%96%D1%84%D0%BE%D0%BD',\n",
       " 'https://ur.wikipedia.org/wiki/%D9%BE%D8%A7%D8%A6%DB%8C%D8%AA%DA%BE%D9%88%D9%86',\n",
       " 'https://vi.wikipedia.org/wiki/Python',\n",
       " 'https://zh.wikipedia.org/wiki/Python_(%E6%B6%88%E6%AD%A7%E4%B9%89)',\n",
       " 'https://www.wikidata.org/wiki/Special:EntityPage/Q747452#sitelinks-wikipedia',\n",
       " 'https://foundation.wikimedia.org/wiki/Privacy_policy',\n",
       " 'https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute',\n",
       " 'https://stats.wikimedia.org/v2/#/en.wikipedia.org',\n",
       " 'https://foundation.wikimedia.org/wiki/Cookie_statement',\n",
       " 'https://wikimediafoundation.org/',\n",
       " 'https://www.mediawiki.org/']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_links=[]\n",
    "for i in link:\n",
    "    if i is None:\n",
    "        next\n",
    "    elif i.startswith('http'):\n",
    "        final_links.append(i)\n",
    "    else:\n",
    "        next\n",
    "        \n",
    "final_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:43:15.692675Z",
     "start_time": "2020-02-11T21:43:15.686262Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:43:18.445657Z",
     "start_time": "2020-02-11T21:43:17.052970Z"
    }
   },
   "outputs": [],
   "source": [
    "#your code\n",
    "html=requests.get(url).content\n",
    "soup=BeautifulSoup(html,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:43:19.004547Z",
     "start_time": "2020-02-11T21:43:18.980770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = soup.find_all(['div'],{'class':['usctitlechanged']})\n",
    "title1 = [title.text.strip().split('\\n') for title in titles]\n",
    "len(title1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:43:21.166836Z",
     "start_time": "2020-02-11T21:43:21.162574Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:43:22.564894Z",
     "start_time": "2020-02-11T21:43:22.287774Z"
    }
   },
   "outputs": [],
   "source": [
    "#your code \n",
    "html=requests.get(url).content\n",
    "soup=BeautifulSoup(html,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:44:45.917723Z",
     "start_time": "2020-02-11T21:44:45.894076Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YASER ABDEL SAID',\n",
       " 'JASON DEREK BROWN',\n",
       " 'ALEXIS FLORES',\n",
       " 'EUGENE PALMER',\n",
       " 'SANTIAGO VILLALBA MEDEROS',\n",
       " 'RAFAEL CARO-QUINTERO',\n",
       " 'ROBERT WILLIAM FISHER',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'ARNOLDO JIMENEZ',\n",
       " 'ALEJANDRO ROSALES CASTILLO']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10 = soup.find_all(['h3'],{'class':['title']})\n",
    "top10_wanted = [wanted.text.strip().split('\\n') for wanted in top10]\n",
    "top10wanted=[]\n",
    "for i in top10_wanted:\n",
    "    top10wanted.append(i[0])\n",
    "\n",
    "top10wanted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T22:32:15.593950Z",
     "start_time": "2020-02-11T22:32:15.588885Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T22:32:17.205774Z",
     "start_time": "2020-02-11T22:32:16.804027Z"
    }
   },
   "outputs": [],
   "source": [
    "#your code\n",
    "html=requests.get(url).content\n",
    "soup=BeautifulSoup(html,'lxml')\n",
    "earthquakes = soup.find('tbody', {'id': 'tbody'}).find_all(\"tr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T22:32:18.418987Z",
     "start_time": "2020-02-11T22:32:18.415182Z"
    }
   },
   "outputs": [],
   "source": [
    "#places=[earthquake(['td'],{'class':['tb_region']}) for earthquake in earthquakes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T22:32:19.514804Z",
     "start_time": "2020-02-11T22:32:19.512170Z"
    }
   },
   "outputs": [],
   "source": [
    "#places=[place[0].text.strip().split('\\n') for place in places]\n",
    "#places2=[]\n",
    "#for i in places:\n",
    "   # places2.append(i[0])\n",
    "\n",
    "#places2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T22:47:43.073845Z",
     "start_time": "2020-02-11T22:47:43.031554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<td class=\"tabev6\"><b><i style=\"display:none;\">earthquake</i><a href=\"/Earthquake/earthquake.php?id=829125\">2020-02-11   22:09:08.6</a></b><i class=\"ago\" id=\"ago0\">22min ago</i></td>,\n",
       " <td class=\"tabev1\">17.95 </td>,\n",
       " <td class=\"tabev2\">N  </td>,\n",
       " <td class=\"tabev1\">66.95 </td>,\n",
       " <td class=\"tabev2\">W  </td>,\n",
       " <td class=\"tabev2\">3.4</td>,\n",
       " <td class=\"tb_region\" id=\"reg0\"> PUERTO RICO</td>]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "places=[earthquake(['td'],{'class':['tabev6','tb_region','tabev1','tabev2']}) for earthquake in earthquakes]\n",
    "places[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T22:54:40.547051Z",
     "start_time": "2020-02-11T22:54:40.540179Z"
    }
   },
   "outputs": [],
   "source": [
    "places3=[place[0].text.split('<a>') for place in places]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T22:56:13.913115Z",
     "start_time": "2020-02-11T22:56:13.905909Z"
    }
   },
   "outputs": [],
   "source": [
    "y=pd.DataFrame(places3)\n",
    "y.columns=['Date']\n",
    "#y[['Date_1','Date_2']] = pd.DataFrame(y.Date.str.split('/n',1),columns =['Y_D','M_D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T22:56:19.883947Z",
     "start_time": "2020-02-11T22:56:19.630287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Lat</th>\n",
       "      <th>N/S</th>\n",
       "      <th>Lon</th>\n",
       "      <th>E/W</th>\n",
       "      <th>Intensity</th>\n",
       "      <th>Place</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[earthquake], [2020-02-11   22:09:08.6]], [2...</td>\n",
       "      <td>[17.95 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[66.95 ]</td>\n",
       "      <td>[W  ]</td>\n",
       "      <td>[3.4]</td>\n",
       "      <td>[ PUERTO RICO]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[earthquake], [2020-02-11   22:04:44.9]], [2...</td>\n",
       "      <td>[19.21 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[155.43 ]</td>\n",
       "      <td>[W  ]</td>\n",
       "      <td>[2.3]</td>\n",
       "      <td>[ ISLAND OF HAWAII, HAWAII]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[earthquake], [2020-02-11   22:03:59.3]], [2...</td>\n",
       "      <td>[38.88 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[16.71 ]</td>\n",
       "      <td>[E  ]</td>\n",
       "      <td>[2.3]</td>\n",
       "      <td>[ SOUTHERN ITALY]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[earthquake], [2020-02-11   22:02:58.9]], [2...</td>\n",
       "      <td>[38.79 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[16.85 ]</td>\n",
       "      <td>[E  ]</td>\n",
       "      <td>[2.3]</td>\n",
       "      <td>[ SOUTHERN ITALY]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[earthquake], [2020-02-11   22:01:18.6]], [3...</td>\n",
       "      <td>[38.82 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[16.87 ]</td>\n",
       "      <td>[E  ]</td>\n",
       "      <td>[3.5]</td>\n",
       "      <td>[ SOUTHERN ITALY]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[[earthquake], [2020-02-11   21:49:22.0]], [4...</td>\n",
       "      <td>[20.87 ]</td>\n",
       "      <td>[S  ]</td>\n",
       "      <td>[70.75 ]</td>\n",
       "      <td>[W  ]</td>\n",
       "      <td>[3.4]</td>\n",
       "      <td>[ OFFSHORE TARAPACA, CHILE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[[[earthquake], [2020-02-11   21:42:44.6]], [4...</td>\n",
       "      <td>[20.94 ]</td>\n",
       "      <td>[S  ]</td>\n",
       "      <td>[70.86 ]</td>\n",
       "      <td>[W  ]</td>\n",
       "      <td>[4.8]</td>\n",
       "      <td>[ OFFSHORE TARAPACA, CHILE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[[earthquake], [2020-02-11   21:36:02.0]], [5...</td>\n",
       "      <td>[0.14 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[130.68 ]</td>\n",
       "      <td>[E  ]</td>\n",
       "      <td>[4.1]</td>\n",
       "      <td>[ PAPUA REGION, INDONESIA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[[earthquake], [2020-02-11   21:31:13.2]], [1...</td>\n",
       "      <td>[17.85 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[66.94 ]</td>\n",
       "      <td>[W  ]</td>\n",
       "      <td>[4.2]</td>\n",
       "      <td>[ PUERTO RICO REGION]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[[earthquake], [2020-02-11   21:21:19.0]], [1...</td>\n",
       "      <td>[16.34 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[98.29 ]</td>\n",
       "      <td>[W  ]</td>\n",
       "      <td>[4.0]</td>\n",
       "      <td>[ OAXACA, MEXICO]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[[[earthquake], [2020-02-11   21:10:51.0]], [1...</td>\n",
       "      <td>[27.90 ]</td>\n",
       "      <td>[S  ]</td>\n",
       "      <td>[69.17 ]</td>\n",
       "      <td>[W  ]</td>\n",
       "      <td>[3.2]</td>\n",
       "      <td>[ ATACAMA, CHILE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[[[earthquake], [2020-02-11   21:04:20.7]], [1...</td>\n",
       "      <td>[4.64 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[83.47 ]</td>\n",
       "      <td>[E  ]</td>\n",
       "      <td>[4.7]</td>\n",
       "      <td>[ NORTH INDIAN OCEAN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[[[earthquake], [2020-02-11   21:02:35.5]], [1...</td>\n",
       "      <td>[40.54 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[20.88 ]</td>\n",
       "      <td>[E  ]</td>\n",
       "      <td>[3.0]</td>\n",
       "      <td>[ ALBANIA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[[[earthquake], [2020-02-11   20:45:16.1]], [1...</td>\n",
       "      <td>[38.86 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[17.43 ]</td>\n",
       "      <td>[E  ]</td>\n",
       "      <td>[2.3]</td>\n",
       "      <td>[ SOUTHERN ITALY]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[[[earthquake], [2020-02-11   20:40:51.1]], [1...</td>\n",
       "      <td>[35.12 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[27.91 ]</td>\n",
       "      <td>[E  ]</td>\n",
       "      <td>[2.9]</td>\n",
       "      <td>[ DODECANESE ISLANDS, GREECE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[[[earthquake], [2020-02-11   20:38:32.0]], [1...</td>\n",
       "      <td>[2.89 ]</td>\n",
       "      <td>[S  ]</td>\n",
       "      <td>[129.85 ]</td>\n",
       "      <td>[E  ]</td>\n",
       "      <td>[3.0]</td>\n",
       "      <td>[ SERAM, INDONESIA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[[[earthquake], [2020-02-11   20:28:46.0]], [2...</td>\n",
       "      <td>[38.45 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[25.68 ]</td>\n",
       "      <td>[E  ]</td>\n",
       "      <td>[2.4]</td>\n",
       "      <td>[ AEGEAN SEA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[[[earthquake], [2020-02-11   20:23:50.8]], [2...</td>\n",
       "      <td>[17.89 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[66.85 ]</td>\n",
       "      <td>[W  ]</td>\n",
       "      <td>[2.7]</td>\n",
       "      <td>[ PUERTO RICO REGION]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[[[earthquake], [2020-02-11   20:15:44.3]], [2...</td>\n",
       "      <td>[38.84 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[122.76 ]</td>\n",
       "      <td>[W  ]</td>\n",
       "      <td>[2.6]</td>\n",
       "      <td>[ NORTHERN CALIFORNIA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[[[earthquake], [2020-02-11   20:15:34.0]], [2...</td>\n",
       "      <td>[20.87 ]</td>\n",
       "      <td>[S  ]</td>\n",
       "      <td>[70.82 ]</td>\n",
       "      <td>[W  ]</td>\n",
       "      <td>[3.0]</td>\n",
       "      <td>[ OFFSHORE TARAPACA, CHILE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[[[earthquake], [2020-02-11   20:15:32.2]], [2...</td>\n",
       "      <td>[19.17 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[155.47 ]</td>\n",
       "      <td>[W  ]</td>\n",
       "      <td>[2.1]</td>\n",
       "      <td>[ ISLAND OF HAWAII, HAWAII]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[[[earthquake], [2020-02-11   20:06:05.5]], [2...</td>\n",
       "      <td>[38.86 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[16.81 ]</td>\n",
       "      <td>[E  ]</td>\n",
       "      <td>[2.0]</td>\n",
       "      <td>[ SOUTHERN ITALY]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[[[earthquake], [2020-02-11   20:02:40.8]], [2...</td>\n",
       "      <td>[35.11 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[27.82 ]</td>\n",
       "      <td>[E  ]</td>\n",
       "      <td>[3.5]</td>\n",
       "      <td>[ DODECANESE ISLANDS, GREECE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[[[earthquake], [2020-02-11   19:55:58.0]], [2...</td>\n",
       "      <td>[2.95 ]</td>\n",
       "      <td>[S  ]</td>\n",
       "      <td>[119.38 ]</td>\n",
       "      <td>[E  ]</td>\n",
       "      <td>[3.9]</td>\n",
       "      <td>[ SULAWESI, INDONESIA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[[[earthquake], [2020-02-11   19:52:36.3]], [2...</td>\n",
       "      <td>[38.45 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[39.16 ]</td>\n",
       "      <td>[E  ]</td>\n",
       "      <td>[2.4]</td>\n",
       "      <td>[ EASTERN TURKEY]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[[[earthquake], [2020-02-11   19:40:50.0]], [2...</td>\n",
       "      <td>[3.32 ]</td>\n",
       "      <td>[S  ]</td>\n",
       "      <td>[135.40 ]</td>\n",
       "      <td>[E  ]</td>\n",
       "      <td>[4.2]</td>\n",
       "      <td>[ PAPUA, INDONESIA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[[[earthquake], [2020-02-11   19:28:44.7]], [3...</td>\n",
       "      <td>[18.85 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[155.21 ]</td>\n",
       "      <td>[W  ]</td>\n",
       "      <td>[2.0]</td>\n",
       "      <td>[ HAWAII REGION, HAWAII]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[[[earthquake], [2020-02-11   19:27:46.0]], [3...</td>\n",
       "      <td>[10.69 ]</td>\n",
       "      <td>[S  ]</td>\n",
       "      <td>[112.58 ]</td>\n",
       "      <td>[E  ]</td>\n",
       "      <td>[4.4]</td>\n",
       "      <td>[ SOUTH OF JAVA, INDONESIA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[[[earthquake], [2020-02-11   19:19:22.3]], [3...</td>\n",
       "      <td>[19.23 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[155.41 ]</td>\n",
       "      <td>[W  ]</td>\n",
       "      <td>[2.2]</td>\n",
       "      <td>[ ISLAND OF HAWAII, HAWAII]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[[[earthquake], [2020-02-11   19:18:26.0]], [3...</td>\n",
       "      <td>[21.69 ]</td>\n",
       "      <td>[S  ]</td>\n",
       "      <td>[68.80 ]</td>\n",
       "      <td>[W  ]</td>\n",
       "      <td>[3.0]</td>\n",
       "      <td>[ ANTOFAGASTA, CHILE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[[[earthquake], [2020-02-11   19:14:33.7]], [3...</td>\n",
       "      <td>[37.90 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[140.18 ]</td>\n",
       "      <td>[E  ]</td>\n",
       "      <td>[4.5]</td>\n",
       "      <td>[ EASTERN HONSHU, JAPAN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[[[earthquake], [2020-02-11   19:08:00.0]], [3...</td>\n",
       "      <td>[18.43 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[68.96 ]</td>\n",
       "      <td>[W  ]</td>\n",
       "      <td>[3.0]</td>\n",
       "      <td>[ DOMINICAN REPUBLIC]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[[[earthquake], [2020-02-11   19:00:19.0]], [3...</td>\n",
       "      <td>[4.83 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[96.47 ]</td>\n",
       "      <td>[E  ]</td>\n",
       "      <td>[3.7]</td>\n",
       "      <td>[ NORTHERN SUMATRA, INDONESIA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[[[earthquake], [2020-02-11   19:00:12.3]], [3...</td>\n",
       "      <td>[36.89 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[121.61 ]</td>\n",
       "      <td>[W  ]</td>\n",
       "      <td>[2.0]</td>\n",
       "      <td>[ CENTRAL CALIFORNIA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[[[earthquake], [2020-02-11   18:58:51.1]], [3...</td>\n",
       "      <td>[35.94 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[27.33 ]</td>\n",
       "      <td>[E  ]</td>\n",
       "      <td>[2.4]</td>\n",
       "      <td>[ DODECANESE ISLANDS, GREECE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[[[earthquake], [2020-02-11   18:46:45.1]], [3...</td>\n",
       "      <td>[18.90 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[155.15 ]</td>\n",
       "      <td>[W  ]</td>\n",
       "      <td>[3.0]</td>\n",
       "      <td>[ HAWAII REGION, HAWAII]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[[[earthquake], [2020-02-11   18:46:11.0]], [3...</td>\n",
       "      <td>[63.86 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[22.54 ]</td>\n",
       "      <td>[W  ]</td>\n",
       "      <td>[3.1]</td>\n",
       "      <td>[ ICELAND REGION]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[[[earthquake], [2020-02-11   18:41:45.6]], [3...</td>\n",
       "      <td>[18.96 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[155.24 ]</td>\n",
       "      <td>[W  ]</td>\n",
       "      <td>[2.7]</td>\n",
       "      <td>[ HAWAII REGION, HAWAII]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[[[earthquake], [2020-02-11   18:36:33.1]], [3...</td>\n",
       "      <td>[61.38 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[150.10 ]</td>\n",
       "      <td>[W  ]</td>\n",
       "      <td>[2.5]</td>\n",
       "      <td>[ SOUTHERN ALASKA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[[[earthquake], [2020-02-11   18:36:06.3]], [3...</td>\n",
       "      <td>[43.99 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[13.55 ]</td>\n",
       "      <td>[E  ]</td>\n",
       "      <td>[3.2]</td>\n",
       "      <td>[ CENTRAL ITALY]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[[[earthquake], [2020-02-11   18:32:08.2]], [3...</td>\n",
       "      <td>[17.87 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[66.88 ]</td>\n",
       "      <td>[W  ]</td>\n",
       "      <td>[2.4]</td>\n",
       "      <td>[ PUERTO RICO REGION]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[[[earthquake], [2020-02-11   18:31:57.0]], [3...</td>\n",
       "      <td>[7.19 ]</td>\n",
       "      <td>[S  ]</td>\n",
       "      <td>[129.46 ]</td>\n",
       "      <td>[E  ]</td>\n",
       "      <td>[4.3]</td>\n",
       "      <td>[ KEPULAUAN BABAR, INDONESIA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[[[earthquake], [2020-02-11   18:29:50.3]], [4...</td>\n",
       "      <td>[19.27 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[155.43 ]</td>\n",
       "      <td>[W  ]</td>\n",
       "      <td>[2.6]</td>\n",
       "      <td>[ ISLAND OF HAWAII, HAWAII]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[[[earthquake], [2020-02-11   18:26:02.1]], [4...</td>\n",
       "      <td>[32.34 ]</td>\n",
       "      <td>[S  ]</td>\n",
       "      <td>[150.93 ]</td>\n",
       "      <td>[E  ]</td>\n",
       "      <td>[2.8]</td>\n",
       "      <td>[ NEW SOUTH WALES, AUSTRALIA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[[[earthquake], [2020-02-11   18:22:53.0]], [4...</td>\n",
       "      <td>[1.95 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[127.05 ]</td>\n",
       "      <td>[E  ]</td>\n",
       "      <td>[3.7]</td>\n",
       "      <td>[ HALMAHERA, INDONESIA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[[[earthquake], [2020-02-11   18:21:05.0]], [4...</td>\n",
       "      <td>[1.17 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[126.35 ]</td>\n",
       "      <td>[E  ]</td>\n",
       "      <td>[3.3]</td>\n",
       "      <td>[ MOLUCCA SEA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[[[earthquake], [2020-02-11   18:15:00.0]], [4...</td>\n",
       "      <td>[2.77 ]</td>\n",
       "      <td>[S  ]</td>\n",
       "      <td>[129.93 ]</td>\n",
       "      <td>[E  ]</td>\n",
       "      <td>[3.5]</td>\n",
       "      <td>[ SERAM, INDONESIA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[[[earthquake], [2020-02-11   18:06:25.0]], [4...</td>\n",
       "      <td>[44.02 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[147.27 ]</td>\n",
       "      <td>[E  ]</td>\n",
       "      <td>[4.2]</td>\n",
       "      <td>[ KURIL ISLANDS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[[[earthquake], [2020-02-11   18:03:16.9]], [4...</td>\n",
       "      <td>[23.40 ]</td>\n",
       "      <td>[S  ]</td>\n",
       "      <td>[179.59 ]</td>\n",
       "      <td>[W  ]</td>\n",
       "      <td>[4.8]</td>\n",
       "      <td>[ SOUTH OF FIJI ISLANDS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[[[earthquake], [2020-02-11   18:02:25.3]], [4...</td>\n",
       "      <td>[44.97 ]</td>\n",
       "      <td>[N  ]</td>\n",
       "      <td>[97.34 ]</td>\n",
       "      <td>[E  ]</td>\n",
       "      <td>[4.6]</td>\n",
       "      <td>[ WESTERN MONGOLIA]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Date       Lat    N/S  \\\n",
       "0   [[[earthquake], [2020-02-11   22:09:08.6]], [2...  [17.95 ]  [N  ]   \n",
       "1   [[[earthquake], [2020-02-11   22:04:44.9]], [2...  [19.21 ]  [N  ]   \n",
       "2   [[[earthquake], [2020-02-11   22:03:59.3]], [2...  [38.88 ]  [N  ]   \n",
       "3   [[[earthquake], [2020-02-11   22:02:58.9]], [2...  [38.79 ]  [N  ]   \n",
       "4   [[[earthquake], [2020-02-11   22:01:18.6]], [3...  [38.82 ]  [N  ]   \n",
       "5   [[[earthquake], [2020-02-11   21:49:22.0]], [4...  [20.87 ]  [S  ]   \n",
       "6   [[[earthquake], [2020-02-11   21:42:44.6]], [4...  [20.94 ]  [S  ]   \n",
       "7   [[[earthquake], [2020-02-11   21:36:02.0]], [5...   [0.14 ]  [N  ]   \n",
       "8   [[[earthquake], [2020-02-11   21:31:13.2]], [1...  [17.85 ]  [N  ]   \n",
       "9   [[[earthquake], [2020-02-11   21:21:19.0]], [1...  [16.34 ]  [N  ]   \n",
       "10  [[[earthquake], [2020-02-11   21:10:51.0]], [1...  [27.90 ]  [S  ]   \n",
       "11  [[[earthquake], [2020-02-11   21:04:20.7]], [1...   [4.64 ]  [N  ]   \n",
       "12  [[[earthquake], [2020-02-11   21:02:35.5]], [1...  [40.54 ]  [N  ]   \n",
       "13  [[[earthquake], [2020-02-11   20:45:16.1]], [1...  [38.86 ]  [N  ]   \n",
       "14  [[[earthquake], [2020-02-11   20:40:51.1]], [1...  [35.12 ]  [N  ]   \n",
       "15  [[[earthquake], [2020-02-11   20:38:32.0]], [1...   [2.89 ]  [S  ]   \n",
       "16  [[[earthquake], [2020-02-11   20:28:46.0]], [2...  [38.45 ]  [N  ]   \n",
       "17  [[[earthquake], [2020-02-11   20:23:50.8]], [2...  [17.89 ]  [N  ]   \n",
       "18  [[[earthquake], [2020-02-11   20:15:44.3]], [2...  [38.84 ]  [N  ]   \n",
       "19  [[[earthquake], [2020-02-11   20:15:34.0]], [2...  [20.87 ]  [S  ]   \n",
       "20  [[[earthquake], [2020-02-11   20:15:32.2]], [2...  [19.17 ]  [N  ]   \n",
       "21  [[[earthquake], [2020-02-11   20:06:05.5]], [2...  [38.86 ]  [N  ]   \n",
       "22  [[[earthquake], [2020-02-11   20:02:40.8]], [2...  [35.11 ]  [N  ]   \n",
       "23  [[[earthquake], [2020-02-11   19:55:58.0]], [2...   [2.95 ]  [S  ]   \n",
       "24  [[[earthquake], [2020-02-11   19:52:36.3]], [2...  [38.45 ]  [N  ]   \n",
       "25  [[[earthquake], [2020-02-11   19:40:50.0]], [2...   [3.32 ]  [S  ]   \n",
       "26  [[[earthquake], [2020-02-11   19:28:44.7]], [3...  [18.85 ]  [N  ]   \n",
       "27  [[[earthquake], [2020-02-11   19:27:46.0]], [3...  [10.69 ]  [S  ]   \n",
       "28  [[[earthquake], [2020-02-11   19:19:22.3]], [3...  [19.23 ]  [N  ]   \n",
       "29  [[[earthquake], [2020-02-11   19:18:26.0]], [3...  [21.69 ]  [S  ]   \n",
       "30  [[[earthquake], [2020-02-11   19:14:33.7]], [3...  [37.90 ]  [N  ]   \n",
       "31  [[[earthquake], [2020-02-11   19:08:00.0]], [3...  [18.43 ]  [N  ]   \n",
       "32  [[[earthquake], [2020-02-11   19:00:19.0]], [3...   [4.83 ]  [N  ]   \n",
       "33  [[[earthquake], [2020-02-11   19:00:12.3]], [3...  [36.89 ]  [N  ]   \n",
       "34  [[[earthquake], [2020-02-11   18:58:51.1]], [3...  [35.94 ]  [N  ]   \n",
       "35  [[[earthquake], [2020-02-11   18:46:45.1]], [3...  [18.90 ]  [N  ]   \n",
       "36  [[[earthquake], [2020-02-11   18:46:11.0]], [3...  [63.86 ]  [N  ]   \n",
       "37  [[[earthquake], [2020-02-11   18:41:45.6]], [3...  [18.96 ]  [N  ]   \n",
       "38  [[[earthquake], [2020-02-11   18:36:33.1]], [3...  [61.38 ]  [N  ]   \n",
       "39  [[[earthquake], [2020-02-11   18:36:06.3]], [3...  [43.99 ]  [N  ]   \n",
       "40  [[[earthquake], [2020-02-11   18:32:08.2]], [3...  [17.87 ]  [N  ]   \n",
       "41  [[[earthquake], [2020-02-11   18:31:57.0]], [3...   [7.19 ]  [S  ]   \n",
       "42  [[[earthquake], [2020-02-11   18:29:50.3]], [4...  [19.27 ]  [N  ]   \n",
       "43  [[[earthquake], [2020-02-11   18:26:02.1]], [4...  [32.34 ]  [S  ]   \n",
       "44  [[[earthquake], [2020-02-11   18:22:53.0]], [4...   [1.95 ]  [N  ]   \n",
       "45  [[[earthquake], [2020-02-11   18:21:05.0]], [4...   [1.17 ]  [N  ]   \n",
       "46  [[[earthquake], [2020-02-11   18:15:00.0]], [4...   [2.77 ]  [S  ]   \n",
       "47  [[[earthquake], [2020-02-11   18:06:25.0]], [4...  [44.02 ]  [N  ]   \n",
       "48  [[[earthquake], [2020-02-11   18:03:16.9]], [4...  [23.40 ]  [S  ]   \n",
       "49  [[[earthquake], [2020-02-11   18:02:25.3]], [4...  [44.97 ]  [N  ]   \n",
       "\n",
       "          Lon    E/W Intensity                           Place  \n",
       "0    [66.95 ]  [W  ]     [3.4]                  [ PUERTO RICO]  \n",
       "1   [155.43 ]  [W  ]     [2.3]     [ ISLAND OF HAWAII, HAWAII]  \n",
       "2    [16.71 ]  [E  ]     [2.3]               [ SOUTHERN ITALY]  \n",
       "3    [16.85 ]  [E  ]     [2.3]               [ SOUTHERN ITALY]  \n",
       "4    [16.87 ]  [E  ]     [3.5]               [ SOUTHERN ITALY]  \n",
       "5    [70.75 ]  [W  ]     [3.4]     [ OFFSHORE TARAPACA, CHILE]  \n",
       "6    [70.86 ]  [W  ]     [4.8]     [ OFFSHORE TARAPACA, CHILE]  \n",
       "7   [130.68 ]  [E  ]     [4.1]      [ PAPUA REGION, INDONESIA]  \n",
       "8    [66.94 ]  [W  ]     [4.2]           [ PUERTO RICO REGION]  \n",
       "9    [98.29 ]  [W  ]     [4.0]               [ OAXACA, MEXICO]  \n",
       "10   [69.17 ]  [W  ]     [3.2]               [ ATACAMA, CHILE]  \n",
       "11   [83.47 ]  [E  ]     [4.7]           [ NORTH INDIAN OCEAN]  \n",
       "12   [20.88 ]  [E  ]     [3.0]                      [ ALBANIA]  \n",
       "13   [17.43 ]  [E  ]     [2.3]               [ SOUTHERN ITALY]  \n",
       "14   [27.91 ]  [E  ]     [2.9]   [ DODECANESE ISLANDS, GREECE]  \n",
       "15  [129.85 ]  [E  ]     [3.0]             [ SERAM, INDONESIA]  \n",
       "16   [25.68 ]  [E  ]     [2.4]                   [ AEGEAN SEA]  \n",
       "17   [66.85 ]  [W  ]     [2.7]           [ PUERTO RICO REGION]  \n",
       "18  [122.76 ]  [W  ]     [2.6]          [ NORTHERN CALIFORNIA]  \n",
       "19   [70.82 ]  [W  ]     [3.0]     [ OFFSHORE TARAPACA, CHILE]  \n",
       "20  [155.47 ]  [W  ]     [2.1]     [ ISLAND OF HAWAII, HAWAII]  \n",
       "21   [16.81 ]  [E  ]     [2.0]               [ SOUTHERN ITALY]  \n",
       "22   [27.82 ]  [E  ]     [3.5]   [ DODECANESE ISLANDS, GREECE]  \n",
       "23  [119.38 ]  [E  ]     [3.9]          [ SULAWESI, INDONESIA]  \n",
       "24   [39.16 ]  [E  ]     [2.4]               [ EASTERN TURKEY]  \n",
       "25  [135.40 ]  [E  ]     [4.2]             [ PAPUA, INDONESIA]  \n",
       "26  [155.21 ]  [W  ]     [2.0]        [ HAWAII REGION, HAWAII]  \n",
       "27  [112.58 ]  [E  ]     [4.4]     [ SOUTH OF JAVA, INDONESIA]  \n",
       "28  [155.41 ]  [W  ]     [2.2]     [ ISLAND OF HAWAII, HAWAII]  \n",
       "29   [68.80 ]  [W  ]     [3.0]           [ ANTOFAGASTA, CHILE]  \n",
       "30  [140.18 ]  [E  ]     [4.5]        [ EASTERN HONSHU, JAPAN]  \n",
       "31   [68.96 ]  [W  ]     [3.0]           [ DOMINICAN REPUBLIC]  \n",
       "32   [96.47 ]  [E  ]     [3.7]  [ NORTHERN SUMATRA, INDONESIA]  \n",
       "33  [121.61 ]  [W  ]     [2.0]           [ CENTRAL CALIFORNIA]  \n",
       "34   [27.33 ]  [E  ]     [2.4]   [ DODECANESE ISLANDS, GREECE]  \n",
       "35  [155.15 ]  [W  ]     [3.0]        [ HAWAII REGION, HAWAII]  \n",
       "36   [22.54 ]  [W  ]     [3.1]               [ ICELAND REGION]  \n",
       "37  [155.24 ]  [W  ]     [2.7]        [ HAWAII REGION, HAWAII]  \n",
       "38  [150.10 ]  [W  ]     [2.5]              [ SOUTHERN ALASKA]  \n",
       "39   [13.55 ]  [E  ]     [3.2]                [ CENTRAL ITALY]  \n",
       "40   [66.88 ]  [W  ]     [2.4]           [ PUERTO RICO REGION]  \n",
       "41  [129.46 ]  [E  ]     [4.3]   [ KEPULAUAN BABAR, INDONESIA]  \n",
       "42  [155.43 ]  [W  ]     [2.6]     [ ISLAND OF HAWAII, HAWAII]  \n",
       "43  [150.93 ]  [E  ]     [2.8]   [ NEW SOUTH WALES, AUSTRALIA]  \n",
       "44  [127.05 ]  [E  ]     [3.7]         [ HALMAHERA, INDONESIA]  \n",
       "45  [126.35 ]  [E  ]     [3.3]                  [ MOLUCCA SEA]  \n",
       "46  [129.93 ]  [E  ]     [3.5]             [ SERAM, INDONESIA]  \n",
       "47  [147.27 ]  [E  ]     [4.2]                [ KURIL ISLANDS]  \n",
       "48  [179.59 ]  [W  ]     [4.8]        [ SOUTH OF FIJI ISLANDS]  \n",
       "49   [97.34 ]  [E  ]     [4.6]             [ WESTERN MONGOLIA]  "
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=pd.DataFrame(places)\n",
    "\n",
    "x.columns=['Date','Lat','N/S','Lon','E/W','Intensity','Place']\n",
    "#x[['Year_Date','Month_Date','Date_Date']] = pd.DataFrame(x.Date.str.split(',',2),columns =['Y_D','M_D','D_D'])\n",
    "x\n",
    "#x['0']=x['0'].str.split(',')\n",
    "#data[\"Team\"]= data[\"Team\"].str.split(\"t\", n = 1, expand = True) \n",
    "#test_names = [name.text.strip().split('\\n') for name in test_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count number of tweets by a given Twitter account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:24:15.159563Z",
     "start_time": "2020-02-11T21:24:12.312Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:24:15.161775Z",
     "start_time": "2020-02-11T21:24:12.314Z"
    }
   },
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** in case account/s name not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:24:15.165476Z",
     "start_time": "2020-02-11T21:24:12.319Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:24:15.167600Z",
     "start_time": "2020-02-11T21:24:12.321Z"
    }
   },
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:24:15.170493Z",
     "start_time": "2020-02-11T21:24:12.324Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:24:15.174481Z",
     "start_time": "2020-02-11T21:24:12.326Z"
    }
   },
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:24:15.177467Z",
     "start_time": "2020-02-11T21:24:12.329Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:24:15.179791Z",
     "start_time": "2020-02-11T21:24:12.332Z"
    }
   },
   "outputs": [],
   "source": [
    "#your code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:24:15.181871Z",
     "start_time": "2020-02-11T21:24:12.336Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:24:15.184490Z",
     "start_time": "2020-02-11T21:24:12.338Z"
    }
   },
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:24:15.186627Z",
     "start_time": "2020-02-11T21:24:12.342Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:24:15.188473Z",
     "start_time": "2020-02-11T21:24:12.344Z"
    }
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:24:15.190382Z",
     "start_time": "2020-02-11T21:24:12.348Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:24:15.192790Z",
     "start_time": "2020-02-11T21:24:12.352Z"
    }
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:24:15.195339Z",
     "start_time": "2020-02-11T21:24:12.355Z"
    }
   },
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:24:15.197314Z",
     "start_time": "2020-02-11T21:24:12.357Z"
    }
   },
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:24:15.201357Z",
     "start_time": "2020-02-11T21:24:12.361Z"
    }
   },
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = city=input('Enter the city:')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:24:15.203765Z",
     "start_time": "2020-02-11T21:24:12.363Z"
    }
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:24:15.205851Z",
     "start_time": "2020-02-11T21:24:12.367Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:24:15.208613Z",
     "start_time": "2020-02-11T21:24:12.370Z"
    }
   },
   "outputs": [],
   "source": [
    "#your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
